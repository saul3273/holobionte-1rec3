# LLMS_OPEN_LOCAL.md
# Modelos de Lenguaje de Gran Escala - Open Source & Local

## üå± Bienvenida a Nuevos Simbiontes

Este documento es un espacio abierto para que el holobionte integre y coordine simbiontes de IA. 
**¬°Invitamos a todos los colaboradores a sumar, editar y expandir esta lista!** 

Si conoces un modelo open source que deber√≠a estar aqu√≠, o tienes experiencia con alguno de los listados, ¬°tu contribuci√≥n es bienvenida! El objetivo es crear un recurso comunitario robusto para deployment local de LLMs.

---

## üìä Tabla de Modelos Open Source

| # | Nombre Modelo | Versi√≥n | Licencia | Repositorio | Peso (Par√°metros) | Optimizaci√≥n Soportada | Hardware / Tuning / Dists | Estado Comunidad | Notas |
|---|---------------|---------|----------|-------------|-------------------|------------------------|---------------------------|------------------|-------|
| 1 | **Llama 3.2** | 3.2 | Llama 3.2 Community | [meta-llama/Llama-3.2](https://github.com/meta-llama/llama-models) | 1B-90B | GGUF, GPTQ, AWQ, FP16 | CPU/GPU, LoRA, PEFT, Ollama/LM Studio | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy activa | Multimodal (vision), excelente para edge devices |
| 2 | **Mistral 7B** | v0.3 | Apache 2.0 | [mistralai/mistral-7b](https://github.com/mistralai/mistral-src) | 7B | GGUF, GPTQ, AWQ | GPU/CPU, QLoRA, Ollama | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy activa | Excelente balance rendimiento/tama√±o |
| 3 | **Phi-3** | 3.5 | MIT | [microsoft/Phi-3](https://github.com/microsoft/Phi-3) | 3.8B-14B | GGUF, ONNX | CPU/GPU/Mobile, ONNX Runtime | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy activa | Optimizado para edge, razonamiento excepcional |
| 4 | **Mixtral 8x7B** | v0.1 | Apache 2.0 | [mistralai/mixtral](https://github.com/mistralai/mistral-src) | 47B (8x7B MoE) | GGUF, GPTQ, ExLlama | GPU (multi), vLLM | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy activa | Mixture of Experts, alto rendimiento |
| 5 | **Qwen 2.5** | 2.5 | Apache 2.0 / Qwen | [QwenLM/Qwen2.5](https://github.com/QwenLM/Qwen2.5) | 0.5B-72B | GGUF, GPTQ, AWQ | GPU/CPU, Ollama, vLLM | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy activa | Multiling√ºe excelente, coding, math |
| 6 | **DeepSeek-V2** | 2 | MIT | [deepseek-ai/DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2) | 236B (21B active) | FP8, GGUF | GPU (multi), vLLM | ‚≠ê‚≠ê‚≠ê‚≠ê Activa | MoE, muy eficiente, economical inference |
| 7 | **Gemma 2** | 2 | Gemma License | [google/gemma-2](https://github.com/google/gemma) | 2B-27B | GGUF, GPTQ, BF16 | CPU/GPU/TPU, Ollama | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy activa | De Google, safety-focused, m√∫ltiples tama√±os |
| 8 | **LLaMA 2** | 2 | Llama 2 Community | [meta-llama/llama](https://github.com/facebookresearch/llama) | 7B-70B | GGUF, GPTQ, AWQ | GPU/CPU, LoRA, Ollama | ‚≠ê‚≠ê‚≠ê‚≠ê Activa | Base s√≥lida, ampliamente fine-tuneado |
| 9 | **Yi-1.5** | 1.5 | Apache 2.0 | [01-ai/Yi-1.5](https://github.com/01-ai/Yi) | 6B-34B | GGUF, GPTQ | GPU/CPU, Ollama | ‚≠ê‚≠ê‚≠ê‚≠ê Activa | Biling√ºe (EN/CN), contexto largo (200K) |
| 10 | **Falcon 180B** | 180B | Apache 2.0 | [tiiuae/falcon-180B](https://huggingface.co/tiiuae/falcon-180B) | 180B | GPTQ, AWQ, FP16 | GPU (multi), vLLM, TGI | ‚≠ê‚≠ê‚≠ê Moderada | Uno de los m√°s grandes open source |
| 11 | **CodeLlama** | 34B | Llama 2 Community | [meta-llama/codellama](https://github.com/facebookresearch/codellama) | 7B-34B | GGUF, GPTQ | GPU/CPU, Ollama | ‚≠ê‚≠ê‚≠ê‚≠ê Activa | Especializado en c√≥digo, fill-in-middle |
| 12 | **Vicuna** | 1.5 | Vicuna License | [lm-sys/FastChat](https://github.com/lm-sys/FastChat) | 7B-33B | GGUF, GPTQ | GPU/CPU, LoRA | ‚≠ê‚≠ê‚≠ê‚≠ê Activa | Chat-optimized, de LMSYS (Chatbot Arena) |
| 13 | **StarCoder2** | 2 | Apache 2.0 | [bigcode/starcoder2](https://github.com/bigcode-project/starcoder2) | 3B-15B | GGUF, FP16 | GPU/CPU, Ollama | ‚≠ê‚≠ê‚≠ê‚≠ê Activa | C√≥digo multilenguaje, context window extenso |
| 14 | **OpenHermes 2.5** | 2.5 | Apache 2.0 | [teknium/OpenHermes-2.5](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B) | 7B | GGUF, GPTQ | GPU/CPU, Ollama | ‚≠ê‚≠ê‚≠ê‚≠ê Activa | Fine-tune de Mistral, excelente para chat |
| 15 | **Orca 2** | 2 | Microsoft Research | [microsoft/Orca-2](https://huggingface.co/microsoft/Orca-2-13b) | 7B-13B | GGUF, GPTQ | GPU/CPU, Ollama | ‚≠ê‚≠ê‚≠ê Moderada | Reasoning-focused, peque√±o pero potente |

---

## üîÆ Espacio para Expansi√≥n (100+ entradas)

*Esta tabla est√° dise√±ada para crecer. A continuaci√≥n hay espacio reservado para nuevas entradas:*

| # | Nombre Modelo | Versi√≥n | Licencia | Repositorio | Peso (Par√°metros) | Optimizaci√≥n Soportada | Hardware / Tuning / Dists | Estado Comunidad | Notas |
|---|---------------|---------|----------|-------------|-------------------|------------------------|---------------------------|------------------|-------|
| 16 | | | | | | | | | |
| 17 | | | | | | | | | |
| 18 | | | | | | | | | |
| 19 | | | | | | | | | |
| 20 | | | | | | | | | |
| ... | *[Espacio para 80+ modelos adicionales]* | | | | | | | | |

---

## üìù Notas y Convenciones

### Licencias Comunes
- **MIT**: Uso comercial libre sin restricciones
- **Apache 2.0**: Uso comercial libre con protecci√≥n de patentes
- **GPL**: Copyleft, derivados deben ser open source
- **Llama Community**: Restricciones en servicios con >700M usuarios
- **Research/Custom**: Revisar t√©rminos espec√≠ficos

### Optimizaciones
- **GGUF**: Formato de llama.cpp, cuantizaci√≥n eficiente para CPU
- **GPTQ**: Cuantizaci√≥n para GPU, mantiene calidad
- **AWQ**: Activation-aware Weight Quantization
- **LoRA/QLoRA**: Fine-tuning eficiente con pocos par√°metros
- **PEFT**: Parameter-Efficient Fine-Tuning

### Plataformas de Deployment
- **Ollama**: CLI simple para correr modelos localmente
- **LM Studio**: GUI para deployment local
- **vLLM**: Inference server de alto rendimiento
- **TGI**: Text Generation Inference (HuggingFace)
- **llama.cpp**: Backend C++ optimizado

### Estado de Comunidad
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy activa: Actualizaciones frecuentes, comunidad grande
- ‚≠ê‚≠ê‚≠ê‚≠ê Activa: Mantenimiento regular
- ‚≠ê‚≠ê‚≠ê Moderada: Actualizaciones ocasionales
- ‚≠ê‚≠ê Baja: Poco mantenimiento
- ‚≠ê Experimental/Archivado

---

## ü§ù C√≥mo Contribuir

### Para Agregar un Nuevo Modelo
1. **Fork** este repositorio
2. Agrega tu modelo a la tabla principal (o rellena las filas vac√≠as)
3. Aseg√∫rate de incluir:
   - Enlace al repositorio oficial
   - Licencia precisa
   - Informaci√≥n actualizada de versi√≥n
   - Notas √∫tiles sobre casos de uso o ventajas
4. Env√≠a un **Pull Request**

### Criterios de Inclusi√≥n
- ‚úÖ Modelo debe ser open source (c√≥digo/pesos disponibles)
- ‚úÖ Debe ser ejecutable localmente
- ‚úÖ Licencia clara y documentada
- ‚úÖ Repositorio activo o comunidad de uso

### Sugerencias de Nuevas Columnas
¬øCrees que falta informaci√≥n importante? ¬°Prop√≥n nuevas columnas!
Ejemplos:
- Contexto m√°ximo
- Fecha de lanzamiento
- Benchmarks (MMLU, HumanEval, etc.)
- Requisitos m√≠nimos de VRAM
- Idiomas soportados

---

## üîó Recursos Adicionales

- [Hugging Face Model Hub](https://huggingface.co/models)
- [Ollama Library](https://ollama.com/library)
- [LM Studio Model Library](https://lmstudio.ai/models)
- [LMSYS Chatbot Arena Leaderboard](https://chat.lmsys.org/?leaderboard)
- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

---

## üåê Licencia de Este Documento

Este archivo `.md` est√° disponible bajo **CC0 1.0 Universal** (dominio p√∫blico). √ösalo, modif√≠calo y comp√°rtelo libremente.

---

**¬°El holobionte crece con cada simbionte que se suma! üå±ü§ñ**

*√öltima actualizaci√≥n: Octubre 2025*
